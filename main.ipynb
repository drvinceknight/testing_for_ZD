{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imp\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import axelrod as axl\n",
    "import axelrod.interaction_utils as iu\n",
    "\n",
    "import testzd as zd\n",
    "\n",
    "C, D = axl.Action.C, axl.Action.D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = imp.load_source('parameters', 'data/raw/parameters.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extortionate zero determinant.\n",
    "\n",
    "In [1], given a match between 2 memory one strategies the concept of Zero Determinant strategies is introduced. It was showed that a player $p\\in\\mathbb{R}^4$ against a player $q\\in\\mathbb{R}^4$ could force a linear relationship between the scores.\n",
    "\n",
    "Assuming the following:\n",
    "\n",
    "- The utilities for player $p$: $S_x = (R, S, T, P)$ and for player $q$: $S_y = (R, T, S, P)$.\n",
    "- The normalised long run score for player $p$: $s_x$ and for player $q$: $s_y$.\n",
    "- Given $p=(p_1, p_2, p_3, p_4)$ a transformed (but equivalent) vector: $\\tilde p=(p_1 - 1, p_2 - 1, p_3, p_4)$, similarly: $\\tilde q=(1 - q_1, 1 - q_2, q_3, q_4)$\n",
    "\n",
    "The main result of [1] is that:\n",
    "\n",
    "if $\\tilde p = \\alpha S_x + \\beta S_y + \\gamma 1$ **or** if $\\tilde q = \\alpha S_x + \\beta S_y + \\gamma 1$ then:\n",
    "\n",
    "$$\n",
    "\\alpha s_x + \\beta s_y + \\gamma 1 = 0\n",
    "$$\n",
    "\n",
    "where $\\alpha, \\beta, \\gamma \\in \\mathbb{R}$\n",
    "\n",
    "As an example consider the `extort-2` strategy defined in [2]. This is given by:\n",
    "\n",
    "$$p=(8/9, 1/2, 1/3, 0)$$\n",
    "\n",
    "Let us use the `Axelrod` library [4, 5] to simulate some matches, here it is against some of the best strategies in the Axelrod library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4240943070730305"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extort2 = axl.ZDExtort2()\n",
    "players = (extort2, axl.EvolvedFSM16())\n",
    "axl.seed(0)\n",
    "match = axl.Match(players, turns=parameters.TURNS)\n",
    "interactions = match.play()\n",
    "scores = match.final_score_per_turn()\n",
    "scores[0] / scores[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0123456790123457"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "players = (extort2, axl.EvolvedANN5())\n",
    "axl.seed(0)\n",
    "match = axl.Match(players, turns=parameters.TURNS)\n",
    "interactions = match.play()\n",
    "scores = match.final_score_per_turn()\n",
    "scores[0] / scores[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3522765038218678"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "players = (extort2, axl.PSOGamblerMem1())\n",
    "axl.seed(0)\n",
    "match = axl.Match(players, turns=parameters.TURNS)\n",
    "interactions = match.play()\n",
    "scores = match.final_score_per_turn()\n",
    "scores[0] / scores[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that `extort2` beats all these strategies. \n",
    "\n",
    "In [1], in fact a specific type of Zero determinant strategy is considered, indeed if: $p_4=0$ then the relationship $\\chi = S_X / S_Y$ holds where $\\chi = \\frac{-\\beta}{\\alpha}$ so that the $S_X$ will be at $\\chi$ times bigger than $S_Y$ as long as $\\chi > 1$.\n",
    "\n",
    "In the case of `extort2` can we reverse engineer $\\alpha$ and $\\beta$?\n",
    "\n",
    "This is equivalent to finding $\\alpha, \\beta, \\gamma \\in \\mathbb{R}$ such that $\\tilde p = \\alpha S_x + \\beta S_y + \\gamma 1$.\n",
    "\n",
    "Recall that $\\tilde p, S_x, S_y, 1\\in\\mathbb{R}^{4\\times 1}$ so this corresponds to a linear system of 4 equations on three variables.\n",
    "\n",
    "$$\\tilde p=Cx$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$\n",
    "C = \\begin{pmatrix}S_x, S_y, 1\\end{pmatrix}\\in\\mathbb{R}^{4\\times 3}\n",
    "$$\n",
    "\n",
    "As an example consider the `extort-2` strategy defined in [2]. This is given by:\n",
    "\n",
    "$$p=(8/9, 1/2, 1/3, 0)$$\n",
    "\n",
    "it is defined to ensure:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\alpha s_x - P &= 2(s_y - P)\\\\\n",
    "\\alpha s_x - 2s_y + P&=0\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Let us solve $Cx=\\tilde p$, we will do this using a least squares approach (which is numerically more efficient than inverting $C$ [3]):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.000000000000003"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = np.array([8 / 9, 1 / 2, 1 / 3, 0])\n",
    "x, SSError = zd.compute_least_squares(p)\n",
    "alpha, beta, gamma = x\n",
    "chi = -beta / alpha\n",
    "chi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that in the case of `extort2` we get a value of $\\chi > 2$ however none of the score ratios above was 2. This is because, in fact the high performing strategies against which `extort2` was playing are not memory one. They still lost their match though.\n",
    "\n",
    "Because of the dimension of $C$: $4\\times 3$ the linear system might not necessarily have a solution, this would be the case if the strategy is not Zero determinant. Thus, using a least squares minimisation approach we are in fact minimising the following value:\n",
    "\n",
    "$$\\text{SSError}=\\|Cx-\\bar p\\|_2^2$$\n",
    "\n",
    "This value: $\\text{SSError}$, in fact gives us a measure of how far from being a Zero determinant strategy a given strategy vector $p$ is.\n",
    "\n",
    "While all strategies are not necessarily memory one: so do not necessarily have a representation as a 4 dimensional vector. There transition rates from all states to any action can still be measured.\n",
    "\n",
    "Let us see how this works, using the 3 strategies above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_p_from_interactions(interactions):\n",
    "    vectors = []\n",
    "    for state_counter in iu.compute_state_to_action_distribution(interactions):\n",
    "        p = []\n",
    "        for state in ((C, C), (C, D), (D, C), (D, D)):\n",
    "            try:\n",
    "                p.append(state_counter[(state, C)] / (state_counter[(state, C)]  + state_counter[(state, D)] ) )\n",
    "            except ZeroDivisionError:\n",
    "                p.append(np.NaN)\n",
    "        vectors.append(p)\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "players = (extort2, axl.EvolvedFSM16())\n",
    "axl.seed(0)\n",
    "match = axl.Match(players, turns=parameters.TURNS)\n",
    "interactions = match.play()\n",
    "p = get_p_from_interactions(interactions=interactions)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3746031746031746,\n",
       " 0.47244094488188976,\n",
       " 0.5443786982248521,\n",
       " 0.5167464114832536]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check how close this strategy is to being ZD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10781562347847042"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, SSError = zd.compute_least_squares(p)\n",
    "SSError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "players = (extort2, axl.EvolvedANN5())\n",
    "axl.seed(0)\n",
    "match = axl.Match(players, turns=parameters.TURNS)\n",
    "interactions = match.play()\n",
    "p = get_p_from_interactions(interactions=interactions)[1]\n",
    "x, SSError = zd.compute_least_squares(p)\n",
    "SSError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This strategy in fact does not visit all states so it is not possible to give a valid calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, nan, 0.8, 0.0]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08277060885461741"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "players = (extort2, axl.PSOGambler2_2_2())\n",
    "axl.seed(0)\n",
    "match = axl.Match(players, turns=parameters.TURNS)\n",
    "interactions = match.play()\n",
    "p = get_p_from_interactions(interactions=interactions)[1]\n",
    "x, SSError = zd.compute_least_squares(p)\n",
    "SSError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it seems that the `PSOGambler2_2_2` is \"more\" ZD than the other two. Note: it is certainly not an extortionate strategy as $p_4 > 0$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.10476190476190476,\n",
       " 0.5183098591549296,\n",
       " 0.0017331022530329288,\n",
       " 0.5051975051975052]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can actually classify all potential extortionate strategies which is Figure 1 of the paper.\n",
    "\n",
    "The paper extends this work to consider a LARGE number of strategies, and identifies if and when strategies actually exhibit extortionate behaviour.\n",
    "\n",
    "We note that the strategies that exhibit strong evolutionary fitness are ones that are able to adapt their behaviour: they do not extort strong strategies (thus cooperation evolves) but they do extort weaker ones. For example, here is a list of strategies against which `EvolvedFSM16` is close to being ZD ($\\text{SSError} < 0.05$) and is close to being extortionate: ($p_4 < 0.05$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vs AntiCycler, chi=0.89, S_X/S_Y=133.47\n",
      "vs Arrogant QLearner, chi=1.27, S_X/S_Y=1.37\n",
      "vs Bush Mosteller: 0.5, 0.5, 3.0, 0.5, chi=0.59, S_X/S_Y=2.47\n",
      "vs Cautious QLearner, chi=1.27, S_X/S_Y=1.37\n",
      "vs Colbert, chi=1.24, S_X/S_Y=5.78\n",
      "vs Hesitant QLearner, chi=1.27, S_X/S_Y=1.37\n",
      "vs Knowledgeable Worse and Worse, chi=1.25, S_X/S_Y=4.12\n",
      "vs Prober 4, chi=0.79, S_X/S_Y=1.0\n",
      "vs Random: 0.5, chi=0.64, S_X/S_Y=5.54\n",
      "vs Risky QLearner, chi=1.27, S_X/S_Y=1.37\n",
      "vs Stochastic Cooperator, chi=0.71, S_X/S_Y=3.14\n",
      "vs ThueMorseInverse, chi=0.95, S_X/S_Y=5.94\n",
      "vs Tranquilizer, chi=2.37, S_X/S_Y=1.01\n",
      "vs Tullock: 11, chi=1.18, S_X/S_Y=0.96\n",
      "vs Worse and Worse, chi=0.97, S_X/S_Y=2.4\n",
      "vs Worse and Worse 2, chi=0.74, S_X/S_Y=0.93\n",
      "vs ZD-Mem2, chi=0.64, S_X/S_Y=1.5\n"
     ]
    }
   ],
   "source": [
    "for opponent in parameters.PLAYER_GROUPS[\"full\"]:\n",
    "    players = (axl.EvolvedFSM16(), opponent)\n",
    "    axl.seed(0)\n",
    "    match = axl.Match(players, turns=parameters.TURNS)\n",
    "    interactions = match.play()\n",
    "    p = get_p_from_interactions(interactions=interactions)[0]\n",
    "    x, SSError = zd.compute_least_squares(p)\n",
    "    if SSError < 0.05 and p[3] < 0.05:\n",
    "        alpha, beta, gamma = x\n",
    "        scores = match.final_score_per_turn()\n",
    "        print(f\"vs {opponent}, chi={round(-beta / gamma, 2)}, S_X/S_Y={round(scores[0] / scores[1], 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This work shows here that not only is there a mathematical basis for suspicion: the calculation of $\\text{SSError}$ but that some high performing strategies seem to exhibit suspicious behaviour that allows them to adapt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Press, William H., and Freeman J. Dyson. \"Iterated Prisoner’s Dilemma contains strategies that dominate any evolutionary opponent.\" Proceedings of the National Academy of Sciences 109.26 (2012): 10409-10413\n",
    "\n",
    "[2] Stewart, Alexander J., and Joshua B. Plotkin. \"Extortion and cooperation in the Prisoner’s Dilemma.\" Proceedings of the National Academy of Sciences 109.26 (2012): 10134-10135.\n",
    "\n",
    "[3] Golub, Gene H., and Charles F. Van Loan. Matrix computations. Vol. 3. JHU Press, 2012.\n",
    "\n",
    "[4] The Axelrod project developers. Axelrod: v4.2.0. 2016. http://doi.org/10.5281/zenodo.1252994\n",
    "\n",
    "[5] Knight, Vincent, et al. \"An Open Framework for the Reproducible Study of the Iterated Prisoner’s Dilemma.\" Journal of Open Research Software 4.1 (2016)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:testing-zd]",
   "language": "python",
   "name": "conda-env-testing-zd-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
