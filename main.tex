% Template for PLoS
% Version 3.5 March 2018
%
% % % % % % % % % % % % % % % % % % % % % %
%
% -- IMPORTANT NOTE
%
% This template contains comments intended 
% to minimize problems and delays during our production 
% process. Please follow the template instructions
% whenever possible.
%
% % % % % % % % % % % % % % % % % % % % % % % 
%
% Once your paper is accepted for publication, 
% PLEASE REMOVE ALL TRACKED CHANGES in this file 
% and leave only the final text of your manuscript. 
% PLOS recommends the use of latexdiff to track changes during review, as this will help to maintain a clean tex file.
% Visit https://www.ctan.org/pkg/latexdiff?lang=en for info or contact us at latex@plos.org.
%
%
% There are no restrictions on package use within the LaTeX files except that 
% no packages listed in the template may be deleted.
%
% Please do not include colors or graphics in the text.
%
% The manuscript LaTeX source should be contained within a single file (do not use \input, \externaldocument, or similar commands).
%
% % % % % % % % % % % % % % % % % % % % % % %
%
% -- FIGURES AND TABLES
%
% Please include tables/figure captions directly after the paragraph where they are first cited in the text.
%
% DO NOT INCLUDE GRAPHICS IN YOUR MANUSCRIPT
% - Figures should be uploaded separately from your manuscript file. 
% - Figures generated using LaTeX should be extracted and removed from the PDF before submission. 
% - Figures containing multiple panels/subfigures must be combined into one image file before submission.
% For figure citations, please use "Fig" instead of "Figure".
% See http://journals.plos.org/plosone/s/figures for PLOS figure guidelines.
%
% Tables should be cell-based and may not contain:
% - spacing/line breaks within cells to alter layout or alignment
% - do not nest tabular environments (no tabular environments within tabular environments)
% - no graphics or colored text (cell background color/shading OK)
% See http://journals.plos.org/plosone/s/tables for table guidelines.
%
% For tables that exceed the width of the text column, use the adjustwidth environment as illustrated in the example table in text below.
%
% % % % % % % % % % % % % % % % % % % % % % % %
%
% -- EQUATIONS, MATH SYMBOLS, SUBSCRIPTS, AND SUPERSCRIPTS
%
% IMPORTANT
% Below are a few tips to help format your equations and other special characters according to our specifications. For more tips to help reduce the possibility of formatting errors during conversion, please see our LaTeX guidelines at http://journals.plos.org/plosone/s/latex
%
% For inline equations, please be sure to include all portions of an equation in the math environment.  For example, x$^2$ is incorrect; this should be formatted as $x^2$ (or $\mathrm{x}^2$ if the romanized font is desired).
%
% Do not include text that is not math in the math environment. For example, CO2 should be written as CO\textsubscript{2} instead of CO$_2$.
%
% Please add line breaks to long display equations when possible in order to fit size of the column. 
%
% For inline equations, please do not include punctuation (commas, etc) within the math environment unless this is part of the equation.
%
% When adding superscript or subscripts outside of brackets/braces, please group using {}.  For example, change "[U(D,E,\gamma)]^2" to "{[U(D,E,\gamma)]}^2". 
%
% Do not use \cal for caligraphic font.  Instead, use \mathcal{}
%
% % % % % % % % % % % % % % % % % % % % % % % % 
%
% Please contact latex@plos.org with any questions.
%
% % % % % % % % % % % % % % % % % % % % % % % %

\documentclass[10pt,letterpaper]{article}
\usepackage[top=0.85in,left=2.75in,footskip=0.75in]{geometry}

% amsmath and amssymb packages, useful for mathematical formulas and symbols
\usepackage{amsmath,amssymb}

% Use adjustwidth environment to exceed column width (see example table in text)
\usepackage{changepage}

% Use Unicode characters when possible
\usepackage[utf8x]{inputenc}

% textcomp package and marvosym package for additional characters
\usepackage{textcomp,marvosym}

% cite package, to clean up citations in the main text. Do not remove.
\usepackage{cite}

% Use nameref to cite supporting information files (see Supporting Information section for more info)
\usepackage{nameref,hyperref}

% line numbers
\usepackage[right]{lineno}

% ligatures disabled
\usepackage{microtype}
\DisableLigatures[f]{encoding = *, family = * }

% color can be used to apply background shading to table cells only
\usepackage[table]{xcolor}

% array package and thick rules for tables
\usepackage{array}

% create "+" rule type for thick vertical lines
\newcolumntype{+}{!{\vrule width 2pt}}

% create \thickcline for thick horizontal lines of variable length
\newlength\savedwidth
\newcommand\thickcline[1]{%
  \noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
  \cline{#1}%
  \noalign{\vskip\arrayrulewidth}%
  \noalign{\global\arrayrulewidth\savedwidth}%
}

% \thickhline command for thick horizontal lines that span the table
\newcommand\thickhline{\noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
\hline
\noalign{\global\arrayrulewidth\savedwidth}}


% Remove comment for double spacing
%\usepackage{setspace} 
%\doublespacing

% Text layout
\raggedright
\setlength{\parindent}{0.5cm}
\textwidth 5.25in 
\textheight 8.75in

% Bold the 'Figure #' in the caption and separate it from the title/caption with a period
% Captions will be left justified
\usepackage[aboveskip=1pt,labelfont=bf,labelsep=period,justification=raggedright,singlelinecheck=off]{caption}
\renewcommand{\figurename}{Fig}

% Use the PLoS provided BiBTeX style
\bibliographystyle{plos2015}

% Remove brackets from numbering in List of References
\makeatletter
\renewcommand{\@biblabel}[1]{\quad#1.}
\makeatother



% Header and Footer with logo
\usepackage{lastpage,fancyhdr,graphicx}
\usepackage{epstopdf}
%\pagestyle{myheadings}
\pagestyle{fancy}
\fancyhf{}
%\setlength{\headheight}{27.023pt}
%\lhead{\includegraphics[width=2.0in]{PLOS-submission.eps}}
\rfoot{\thepage/\pageref{LastPage}}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrule}{\hrule height 2pt \vspace{2mm}}
\fancyheadoffset[L]{2.25in}
\fancyfootoffset[L]{2.25in}
\lfoot{\today}

%% Include all macros below

\newcommand{\lorem}{{\bf LOREM}}
\newcommand{\ipsum}{{\bf IPSUM}}

%% END MACROS SECTION

\newcommand{\SSe}{\text{SSE}}


\begin{document}
\vspace*{0.2in}

% Title must be 250 characters or less.
\begin{flushleft}
{\Large
\textbf\newline{Recognising and evaluating the effectiveness
       of extortion in the Iterated Prisoner's Dilemma} % Please use "sentence case" for title and headings (capitalize only the first word in a title (or heading), the first word in a subtitle (or subheading), and any proper nouns).
}
\newline
% Insert author names, affiliations and corresponding author email (do not include titles, positions, or degrees).
\\
Vincent A. Knight\textsuperscript{1,\Yinyang *},
Marc Harper\textsuperscript{2\Yinyang},
Nikoleta E. Glynatsi\textsuperscript{3\Yinyang},
Jonathan Gillard\textsuperscript{2\ddag},
% Name6 Surname\textsuperscript{2\ddag},
% Name7 Surname\textsuperscript{1,2,3*},
% with the Lorem Ipsum Consortium\textsuperscript{\textpilcrow}
\\
\bigskip
\textbf{1} Cardiff University, School of Mathematics, Cardiff, United Kingdom
\\
\textbf{2} Google Inc., Mountain View, CA, United States of America
\\
\textbf{3} Max Planck Research Group on the Dynamics of Social Behavior, Max Planck Institute for Evolutionary Biology, Pl\"on, Germany
\\
\bigskip

% Insert additional author notes using the symbols described below. Insert symbol callouts after author names as necessary.
% 
% Remove or comment out the author notes below if they aren't used.
%
% Primary Equal Contribution Note
\Yinyang These authors contributed equally to this work.

% Additional Equal Contribution Note
% Also use this double-dagger symbol for special authorship notes, such as senior authorship.
\ddag These authors also contributed equally to this work.

% Use the asterisk to denote corresponding authorship and provide email address in note below.
* knightva@cardiff.ac.uk

\end{flushleft}
% Please keep the abstract below 300 words
\section*{Abstract}
Establishing and maintaining mutual cooperation in agent-to-agent interactions
can be viewed as a question of direct reciprocity and
readily applied to the Iterated Prisoner's Dilemma. Agents cooperate, at a
small cost to themselves, in the hope of obtaining a future benefit.
Zero-determinant strategies, introduced in 2012, have a subclass of
strategies that are provably extortionate. In the established literature,
most of the studies of the effectiveness or lack
thereof, of zero-determinant strategies is done by placing some
zero-determinant strategy in a specific scenario (collection of agents) and
evaluating its performance either numerically or theoretically.

Extortionate strategies are algebraically rigid and memory-one by definition,
and requires complete knowledge of a strategy (the memory-one cooperation
probabilities). The contribution of this work is a method to detect
extortionate behaviour from the history of play of an arbitrary strategy. This inverts
the paradigm of most studies: instead of observing the effectiveness of some
theoretically extortionate strategies, the largest known collection of
strategies will be observed and their intensity of extortion quantified empirically.
Moreover, we show that the lack of adaptability of extortionate strategies
extends via this broader definition.


\linenumbers

% Use "Eq" instead of "Equation" for equation citations.
\section*{Introduction}

The Iterated Prisoner's Dilemma (IPD) is a model for rational and evolutionary
interactive behaviour, having applications in biology, the study of human social
behaviour, and many other domains. A standard representation  of the game is
given in Eq~\ref{eqn:theipd}, where the constraints ensure a
non-cooperative equilibrium.

\begin{equation}
    \begin{pmatrix}
        R & S \\
        T & P
    \end{pmatrix}
    \qquad
    T > R > P > S\text{ and }2 R > T + S
    \label{eqn:theipd}
\end{equation}

The parameters of~(\ref{eqn:theipd}) correspond to:

\begin{itemize}
    \item \(R\): The reward for both players cooperating.
    \item \(T\): The temptation value of defecting.
    \item \(S\): The sucker value of cooperating against a defection.
    \item \(P\): The punishment value when both players defect.
\end{itemize}

Early work in the field~\cite{Axelrod1980, Axelrod1980a} showed that cooperative behaviour
could be successful in repeated interactions: Tit For Tat performed
strongly in a tournament of strategies with various degrees of non-cooperation.
The simplicity of Tit For Tat, which only requires knowledge of the
opponent's previous play, led to much research concentrating on these so called
memory-one strategies. A bibliometric study of the literature on the IPD is available
at~\cite{glynatsi2021bibliometric}.

A subclass of memory-one strategies known as zero-determinant (ZD) strategies
were introduced in \cite{Press2012}. Of these, extortionate strategies have received
considerable interest in the literature~\cite{hilbe2015partners}. These
strategies ``enforce'' a difference in stationary payouts between themselves and
their opponents. The definition requires a precise algebraic relationship
between the probabilities of cooperation given the outcome of the previous round
of play. Slight alterations to these probabilities can cause a strategy to no
longer satisfy the necessary relations to be considered extortionate.

In~\cite{adami2013evolutionary, hilbe2018partners, Hilbe2013, hilbe2013adaptive,
hilbe2015partners, ichinose2018zero, Moran1707} the effectiveness of these
strategies in an evolutionary setting was discussed. For
example~\cite{adami2013evolutionary} showed that ZD strategies were not
evolutionarily stable. Furthermore, in that work it was also postulated that
`evolutionarily successful ZD strategies could be designed that use longer
memory to distinguish self from non-self'. In~\cite{lee2015art} long memory
strategies are designed that are able to self recognise and in~\cite{Moran1707}
evolutionary processes showed the emergence of similar abilities.
In~\cite{hilbe2018partners} two sets of strategies are identified: partners and
rivals and some discussion about the environments necessary for either to be
evolutionary stable are given. In a non-evolutionary context, the work of~\cite{becks2019extortion} uses social
experiments to suggest that higher rewards promote extortionate
behaviour, where statistical techniques are used to identify such behaviour.

The algebraic relationships of extortion, discussed in
the Methods section, define a subspace of
\(\textbf{p}\in\mathbb{R}^4\) which can be used to broaden the definition of an extortionate
strategy by requiring only that the defining four cooperation probabilities of a
memory-one strategy are close to an algebraically extortionate strategy, by the usual
technique of orthogonal projection. Moreover, given the history of play of a
strategy in an actual matchup, we can empirically observe its four
cooperation probabilities, measure the distance to the subspace of extortionate
strategies, and use this distance as a measure of the extortionality of a
strategy. This method can be applied to any strategy regardless of the memory
depth and avoids the algebraic rigidity and instability issues.

We apply this method to the largest known corpus of strategies for the iterated
prisoner's dilemma (the Axelrod Python library~\cite{Knight2016, Knight2018})
and validate empirically that the method in fact detects extortionate strategies.
A large tournament with 204 strategies demonstrates that sophisticated
strategies can in fact recognise extortionate behaviour and adapt to their
opponents. Further, statistical analysis of these strategies in the context of
evolutionary dynamics demonstrates the importance of adaptability to achieve
evolutionary stability. All of the code and data discussed in
Results section is open sourced, archived, and written
according to best scientific principles~\cite{Wilson2014}. The data archive can
be found at~\cite{vincent_knight_2018_1297075} and the source code was developed
at~\url{https://github.com/drvinceknight/testing_for_ZD/} and has been archived
at~\cite{vincent_knight_2019_2598534}. This large tournament is complemented
with evolutionary dynamics that offer some insight into the
effectiveness of extortionate strategies.

Several theoretical insights emerge from this work. Infamously, extortionate
strategies do not play well with themselves. In \cite{Press2012},
Press and Dyson claim that a player with a ``theory of mind'' would
rationally chose to cooperate against an opponent that also has knowledge
of zero-determinant strategies to avoid sustained mutual defection. While not
possible for memory-one strategies, we show that this behavior is exhibited by
relatively simple longer memory strategies which previously emerged from an
evolutionary selection process. Similarly, in
\cite{adami2013evolutionary}, Adami and Hintze suggest that there may exist
strategies that are able to selectively behave extortionately to some opponents
and cooperatively to others. We show that this is indeed the case for the same
evolved strategies. It seems that humans have trouble explicitly creating such
strategies but evolution is able to do so by optimizing for total payoff in IPD
interactions. Accordingly, while resistance to extortionate behavior appears
critical to the evolution of cooperation, there is no prohibition on selectively
extorting weaker opponents, even in population dynamics, and this behavior is
evolutionarily advantageous.

\section*{Materials and methods}
\subsection*{Recognising extortion}

This section reviews the definition of ZD
strategies from the literature, present the vector space in which such
strategies exist and finally present a novel measure that allows for a measure
of how far any memory-one strategy is from the space of ZD
strategies. Note that in this section no claims about the evolutionarily
effectiveness of such strategies are made.

ZD strategies are a special case of memory-one strategies,
which are defined by elements of \(\mathbb{R}^4\), mapping a state of
\({\{C, D\}}^2\), corresponding to the prior round of play, to a probability of
cooperating in the next round. A match between two such strategies creates a
Markov chain with transient states \({\{C, D\}}^2\). The main result
of~\cite{Press2012} is that given two memory-one players \(\textbf{p},
\textbf{q}\in\mathbb{R}^4\), a linear relationship between the players' scores can, in
some cases, be forced by one of the players for specific choices of these
probabilities.

Using the notation of~\cite{Press2012}, the utilities for player \(X\) (playing
the strategy \(\textbf{p}\))
are given by \(\textbf{S}_x=(R, S, T, P)\) and for player \(Y\) (playing the
strategy \(\textbf{q}\) by \(\textbf{S}_y=(R, T, S, P)\)
and the stationary scores of each player are given by \(S_X\) and \(S_Y\)
respectively. The main result of~\cite{Press2012} is that if

\begin{equation}\label{eqn:linear_relationship_for_p}
    \tilde{\textbf{p}}=\alpha \textbf{S}_x + \beta \textbf{S}_y + \gamma
\end{equation}

or

\begin{equation}\label{eqn:linear_relationship_for_q}
    \tilde{\textbf{q}}=\alpha \textbf{S}_x + \beta \textbf{S}_y + \gamma
\end{equation}

where \(\tilde{\textbf{p}} = (p_1 - 1, p_2 - 1, p_3, p_4)\) and
\(\tilde{\textbf{q}} = (q_1 - 1, q_3, q_2 - 1, q_4)\) then:

\begin{equation}
    \alpha S_X + \beta S_Y + \gamma = 0
\end{equation}

Extortionate strategies are defined as follows. If this relationship is
satisfied

\begin{equation}\label{eqn:constraint_for_extortion}
    \gamma = - P(\alpha + \beta)
\end{equation}

then the player can ensure \((S_X - P)=\chi(S_Y-P)\) where:

\begin{equation}\label{eqn:definition_of_chi}
    \chi=\frac{-\beta}{\alpha}
\end{equation}

\noindent Thus, if Eq (\ref{eqn:constraint_for_extortion}) holds and \(\chi >1\) then a player is
said to extort their opponent.
Next, the reverse problem is considered: given a
\(\textbf{p}\in\mathbb{R}^4\) can one determine if the associated strategy is attempting
to act in an extortionate way?

\subsection*{Subspace of Extortionate Strategies}

Constraints (\ref{eqn:linear_relationship_for_p}) and
(\ref{eqn:constraint_for_extortion}) correspond to:

\begin{align}
    \tilde p_1 & = \alpha R + \beta R - P (\alpha + \beta)
            \label{eqn:condition_for_tilde_p1}\\
    \tilde p_2 & = \alpha S + \beta T - P (\alpha + \beta)
            \label{eqn:condition_for_tilde_p2}\\
    \tilde p_3 & = \alpha T + \beta S - P (\alpha + \beta)
            \label{eqn:condition_for_tilde_p3}\\
    \tilde p_4 & = \alpha P + \beta P - P (\alpha + \beta) = 0
            \label{eqn:condition_for_tilde_p4}
\end{align}

Eq(\ref{eqn:condition_for_tilde_p4}) ensures that \(p_4=\tilde p_4=0\).
Eqs (\ref{eqn:condition_for_tilde_p1}-\ref{eqn:condition_for_tilde_p3})
can be used to eliminate \(\alpha, \beta\), giving:

\begin{equation}\label{eqn:planar_definition_of_extortion}
    \tilde p_1 = \frac{(R - P)(\tilde p_2 + \tilde p_3)}{S + T - 2P}
\end{equation}

with:

\begin{equation}\label{eqn:definition_of_chi}
    \chi = \frac{\tilde p_2 (P - T) + \tilde p_3 (S - P)}
                {\tilde p_2 (P - S) + \tilde p_3 (T - P)}
\end{equation}

Given a strategy \(p\in\mathbb{R}^{4}\) Eqs
(\ref{eqn:condition_for_tilde_p4}-\ref{eqn:definition_of_chi}) can be used to
check if a strategy is extortionate. The conditions correspond to:

\begin{align}
    p_1 & = \frac{(R-P)(p_2 + p_3) - R + T + S - P}{S + T - 2P}
     \label{eqn:condition_for_p1}\\
    p_4 & = 0 \label{eqn:condition_for_p4}\\
    p_2 + p_3 & < 1\label{eqn:condition_for_chi}
\end{align}

The algebraic steps necessary to prove these results are available in the
supporting materials, and note that an equivalent formulation was obtained
in~\cite{adami2013evolutionary}.

Based on Eqs (\ref{eqn:condition_for_p1}-\ref{eqn:condition_for_chi}), 
it is evident that all extortionate strategies
reside on a triangular plane within a three-dimensional space.
Using this formulation it can be seen that a necessary (but not sufficient)
condition for an extortionate strategy is that it cooperates on average less
than 50\% of the time when in a state of disagreement with the opponent
(\ref{eqn:condition_for_chi}).

As an example, consider the known extortionate strategy \(\textbf{p}=(8 / 9, 1 / 2, 1 /
3, 0)\) from~\cite{Stewart2012}, which is referred to as Extort-2. In
this case, for the standard values of \((R, S, T, P) = (3, 0, 5, 1)\)
constraint (\ref{eqn:condition_for_p1}) corresponds to:

\begin{equation}
    p_1 = \frac{2(p_2 + p_3) + 1}{3}
        = \frac{2(1 / 2 + 1 / 3) + 1}{3}
        = \frac{8}{9}
\end{equation}

It is clear that in this case all constraints hold. As a counterexample,
consider the strategy that cooperates 25\% of the time: \(\textbf{p}=(1 /4, 1 / 4, 1 / 4,
1 / 4)\) satisfies~(\ref{eqn:condition_for_chi}) but is not extortionate as:

\begin{equation}
    p_1 \ne \frac{2(p_2 + p_3) + 1}{3}
        = \frac{2(1 / 4 + 1 / 4) + 1}{3}
        = \frac{2}{3}
\end{equation}

\subsection*{Measuring Extortion from the History of Play}

Not all strategies are memory-one strategies but it is possible to
measure a given \(\textbf{p}\) from any set of interactions between two strategies.
This approach can then be used to confirm that a given strategy is acting
in an extortionate manner even if it is not a memory-one strategy. However, in
practice, if an exact form for \(\textbf{p}\) is not known but measured from observed
plays of the game then measurement and/or numerical error might lead to an
extortionate strategy not being confirmed as such. \footnote{Comparing theoretic
and actual plays of the IPD is not novel, see for example~\cite{Rand2013}.}


As an example consider Table~\ref{tab:actual_plays_of_ZDextort-2}, which shows
some actual plays of Extort-2 (\(\textbf{p}=(8 / 9, 1 / 2, 1 / 3, 0)\)) against an
alternating strategy (\(\textbf{p}=(0, 0, 1, 1)\)). In this particular instance the
measured value of \(\textbf{p}\) for the known extortionate strategy would be:
\((2/2, 1/5, 3/8, 0/4)\) which does not fit the definition of a ZD strategy.


\begin{table}[!ht]
    \begin{adjustwidth}{-2.25in}{0in} % Comment out/remove adjustwidth environment if table fits in text column.
    \centering
    \caption{
        {\bf A seeded play of 20 turns of two strategies.}}
    \label{tab:actual_plays_of_ZDextort-2}
        \begin{tabular}{lllllllllllllllllllll}
            \hline
            Turn               &  1 &  2 &  3 &  4 &  5 &  6 &  7 &  8 &  9 & 10 &  11 &  12 &  13 &  14 &  15 &  16 &  17 &  18 &  19 &  20 \\ \thickhline
            (8/9, 1/2, 1/3, 0) &  C &  C &  D &  D &  D &  C &  D &  D &  D & D &   D &   C &   C &   C &   D &   D &   D &   C &   D &   D \\ \hline
            Alternator         &  C &  D &  C &  D &  C &  D &  C &  D &  C & D &   C &   D &   C &   D &   C &   D &   C &   D &   C &   D \\\hline
        \end{tabular}
    \end{adjustwidth}
\end{table}


Note that measurement of behaviour might in some cases lead to missing values.
For example the strategy \(\textbf{p}=(8 / 9, 1 / 2, 1 / 3, 0)\) when playing against an
opponent that always cooperates will in fact never visit any state which would allow measurement
of \(p_3\) and \(p_4\). To overcome this, it is proposed that if \(s\) is a state
that is not visited then \(p_s\) is approximated using a sensible prior or
imputation. In Section~\ref{sec:numerical-experiments} the overall cooperation
rate is used. Another approach to overcoming this measurement error would be to
measure strategies in a sufficiently noisy environment.

We can measure how close a strategy is to being zero determinant using standard
linear algebraic approaches. Essentially we attempt to find \(\textbf{x}=(\alpha,
\beta)\) such that:

\begin{equation}\label{eqn:linear_algebraic_equation_for_p}
    C\textbf{x}= \tilde{\textbf{p}}
\end{equation}

where \(C\) corresponds to Eqs
(\ref{eqn:condition_for_tilde_p1}-\ref{eqn:condition_for_tilde_p3}) and is
given by:

\begin{equation}\label{eqn:definition_of_C}
    C =
    \begin{bmatrix}
        R - P & R- P \\
        S - P & T- P \\
        T - P & S- P \\
        0     & 0 \\
    \end{bmatrix}
\end{equation}

Note that in general, Eq (\ref{eqn:linear_algebraic_equation_for_p}) will
not necessarily have a solution. From the Rouch\'{e}-Capelli theorem if there is
a solution it is unique since \(\text{rank}(C)=2\) which is the dimension of the
variable \(\textbf{x}\). The best fitting \(\textbf{x}^*\) is defined by:

\begin{equation}\label{eqn:x_star}
    \textbf{x}^* = \text{argmin}_{\textbf{x}\in\mathbb{R}^2}\|C \textbf{x}- \tilde{\textbf{p}}\|_2^2
\end{equation}

Known results~\cite{kutner2004applied, rao1973linear, wakefield2013bayesian}
yield \(\textbf{x}^*\), corresponding to the nearest extortionate strategy to the measured
\(\tilde{\textbf{p}}\). It is in fact an orthogonal projection of
\(\tilde{\textbf{p}}\) on to the
plane defined by (\ref{eqn:condition_for_p1}).

\begin{equation}\label{eqn:x_star_formula}
    \textbf{x}^* = {\left(C^{T}C\right)}^{-1}C^{T}\tilde{\textbf{p}}
\end{equation}

The squared norm of the remaining error is referred to as sum of squared errors
of prediction (\(\SSe\)):

\begin{equation}\label{eqn:r_squared}
    \SSe = \|C x^*- \tilde p\|_2^2
\end{equation}

This gives expressions for \(\alpha, \beta\) as \(\alpha=x^*_1\) and
\(\beta=x^*_2\) thus the conditions for a strategy to be acting extortionately
becomes:

\begin{equation}
    \frac{-x^*_2}{x^*_1} = \chi > 1 \text{ and }x^*_1\ne0
        \label{eqn:measured_condition_for_chi}
\end{equation}

A further known result~~\cite{kutner2004applied, rao1973linear,
wakefield2013bayesian} gives an expression for
\(\SSe\):

\begin{align}\label{eqn:x_SSError_formula}
    \SSe &= {\tilde{\textbf{p}}} ^ T \tilde{\textbf{p}}
           \tilde{\textbf{p}} C \left(C ^ T C \right) ^ {-1} C ^ T
           \tilde{\textbf{p}} \\
    \SSe &={\tilde{\textbf{p}}} ^ T \tilde{\textbf{p}} - \tilde{\textbf{p}} C \textbf{x}^*
\end{align}

Using this approach, the memory-one representation \(\textbf{p}\in\mathbb{R}^4\) of any
strategy against any other can can be measured and if
(\ref{eqn:measured_condition_for_chi}) holds then (\ref{eqn:x_SSError_formula})
can be used to identify if a strategy is acting extortionately. While the
specific memory-one representation might not be one that acts extortionately or
is even feasible (as noted in~\cite{Press2012}), a
high \(\SSe\) does imply that a strategy is not extortionate. For a measured
\(\textbf{p}\), \(\SSe\) corresponds to the best fitting \(\alpha, \beta\). Suspicion of
extortion then corresponds to a threshold on \(\SSe\) and a comparison of the
measured \(\chi=\frac{-\beta}{\alpha}\).

\section*{Results}

This section validates the approach of the previous section and
present a number of numerical experiments to identify if strategies that perform
strongly in evolutionary settings are close or not to the space of
ZD strategies.

\subsection*{Validation}

To validate the method described, we use~\cite{Stewart2012} which
presents results from a tournament with
19 strategies
with specific consideration given to ZD strategies. This
tournament is reproduced here using the Axelrod-Python
library~\cite{Knight2016}. To obtain a good measure of the corresponding
transition rates for each strategy all matches have been run for
2000 turns and every match has been
repeated 60 times. All of this
interaction data is available at~\cite{vincent_knight_2018_1297075}. Note that
in the interest of open scientific practice,~\cite{vincent_knight_2018_1297075}
also contains interaction data for noisy and probabilistic ending interactions
which are not investigated here.

Fig~\ref{fig:sserror_in_stewart_plotkin} shows the
\(\SSe\) values for all the strategies in the tournament, as
reported in~\cite{Stewart2012} the extortionate strategy Extort-2 gains a large number of
wins. Notice that the mean \(\SSe\) for Extort-2 is approximately zero, while for
the always cooperating strategy Cooperator the \(\SSe\) is far from zero. It is
also clear that ZD-GTFT2 defined as a ZD strategy does not act
extortionately. This is evident by the fact that it does not rank highly according
to wins which is due to its value of \(\chi\) being less than 1.
ZD-GTFT2 is referred to as a ``generous'' ZD strategy, other examples of this
include ZDGen2 and ZDSet2 defined in~\cite{sep-prisoner-dilemma}. The
general performance of these will be discussed in
the Results section.

\begin{figure}[!htbp]
    \centering
    \caption{{\bf\(\SSe\) and best fitting \(\chi\) for~\cite{Stewart2012},
        ordered (in descending order) both by number of wins and overall score.
        A win is when a strategy obtains a higher score than the player it is
        interacting with.
        The strategies with a positive skew
        \(\SSe\) and high \(\chi\) win the most matches, although even the
        theoretic
        extortionate strategy does not act in a perfectly extortionate manner in
        all matches. The strategies with a high score have a negatively skewed
        \(\SSe\).
        }}
    \label{fig:sserror_in_stewart_plotkin}
\end{figure}

Next, the results of a much larger tournament are presented.
As a final validation of the proposed methodology here,
Table~\ref{tbl:chi_versus_observed_chi_for_zd} shows the theoretic values of
\(\chi\) versus the measured values for all ZD strategies in the tournament.
The method accurately recovers \(\chi\) from the observed play of
the strategies. Furthermore, the \(\SSe\) value is low for all of these. The
values of \(\SSe\) above 1 indicate that whilst these strategies are designed to
act extortionately they do not do so in all cases. This will be discussed in
more detail in the next section.

\begin{table}[!ht]
    \centering
    \caption{
        {\bf Validating the approach by comparing the measured values of
    \(\chi\) and the theoretic values of
    \(\chi\) for all ZD strategies in the larger tournament. The value of
    \(\chi\) is effectively recovered from observed play and the \(\SSe\)
    indicates that not all strategies are able to play as expected all the
time.}}
    \label{tbl:chi_versus_observed_chi_for_zd}

\begin{tabular}{lrrr}
\hline
                  Name &  Measured chi &  Theoretic chi &     SSE \\
\thickhline
         Firm But Fair &        1.0000 &     1.0000 &  0.4446 \\
                  GTFT &        0.6999 &     0.7000 &  0.1373 \\
                  Joss &        1.2428 &     1.2431 &  0.0006 \\
             Soft Joss &        0.9110 &     0.9112 &  0.0123 \\
 Stochastic Cooperator &        3.0248 &     3.0276 &  0.2158 \\
       Stochastic WSLS &       12.6105 &    12.6000 &  1.0627 \\
   Win-Shift Lose-Stay &        1.8333 &     1.8333 &  1.4706 \\
   Win-Stay Lose-Shift &       16.0000 &    16.0000 &  1.2353 \\
          ZD-Extortion &       10.0067 &    10.0000 &  0.0000 \\
           ZD-Extort-2 &        1.9978 &     2.0000 &  0.0000 \\
            ZD-Extort3 &        3.0022 &     3.0000 &  0.0000 \\
        ZD-Extort-2 v2 &        2.0020 &     2.0000 &  0.0000 \\
           ZD-Extort-4 &        3.9998 &     4.0000 &  0.0000 \\
             ZD-GTFT-2 &        0.8887 &     0.8889 &  0.0662 \\
              ZD-GEN-2 &        0.8892 &     0.8889 &  0.0165 \\
              ZD-SET-2 &        2.4022 &     2.4000 &  0.0661 \\
\hline
        \end{tabular}
\end{table}


\subsection*{Numerical experiments}

Next we investigate a tournament with
204 strategies. The results of
this analysis are shown in Fig\ref{fig:sse_chi_probabilities_in_full}. The
top ranking strategies by number of wins act in an extortionate way (but not
against all opponents) and it can be seen that a small subgroup of strategies
achieve mutual defection.  All the top ranking strategies according to score
do not extort each other, however they
\textbf{do} exhibit extortionate behaviour towards a number of the lower ranking
strategies.

\begin{figure}[!htbp]
    \centering
    \caption{{\bf \(\SSe\) and probability of mutual defection (\(P(DD)\)) 
        for the strategies for
        the full tournament. The strategies with high number of wins
        have a low \(\SSe\) however are often locked in mutual defection as
        evidenced by a high \(P(DD)\). The strategies with a high score
        have a high \(\SSe\) against the other high scoring strategies
        indicating that fixed linear relationship is being enforced. However
        against the low scoring strategies they have a lower \(\SSe\) and
        against the very lowest scoring strategies a high \(P(DD)\).}}
    \label{fig:sse_chi_probabilities_in_full}
\end{figure}

Note that while a strategy may attempt to act extortionately, not all opponents
can be effectively extorted. For example, a strategy that always defects never
receives a lower score than its opponent. As defined by~\cite{Press2012}, an
extortionate ZD strategy will mutually defect with such an opponent which
corresponds to the high values of \(P(DD)\) seen in
Fig~\ref{fig:sse_chi_probabilities_in_full} the top left quadrant.

A detailed look at selected strategies is given in
Table~\ref{tbl:overall_summary_results}. The high scoring strategies presented
have a negatively skewed \(\SSe\) whilst the ZD strategies have a low score but
high probability of winning and higher probability of mutual defection.
The skew of \(\SSe\) of all strategies is shown in
Fig~\ref{fig:sserror_in_std} and supports the
same conclusion. This evidences an idea proposed
in~\cite{adami2013evolutionary}: sophisticated strategies are able to recognise
their opponent and defend themselves against extortion.  The high ranking
strategies were in fact trained to maximise score~\cite{Harper2017} which seems
to have created strategies able to extort weaker strategies whilst cooperating
with stronger ones. Indeed unconditional extortion is self defeating.

\begin{table}[!hbtp]
    \begin{adjustwidth}{-2.25in}{0in} % Comment out/remove adjustwidth environment if table fits in text column.
    \centering
    \caption{
        {\bf Summary of results for a selected list of strategies. Similarly to
        Fig~\ref{fig:sserror_in_stewart_plotkin}, the high scoring strategies
        have a negatively skewed \(\SSe\). The strategies with a
        large number of wins have a low \(\SSe\) and positively skewed
        \(\SSe\). Note that a value of \(\chi=0.063\) and \(\SSe=1.235\)
        corresponds to a vector \(p=(1,1,1,1)\) which highlights that the high
        scoring strategies, adapt and in fact cooperate often.}}
    \label{tbl:overall_summary_results}
    \small
\begin{tabular}{rlrrrrrrr}
\hline
 Rank &                  Name &  Score per turn &  $P($Win$)$ &  P(DD) &  Median $\chi$ &  Mean SSE &  Skew SSE &  Var SSE \\
\thickhline
    1 &  EvolvedLookerUp2\_2\_2 &           2.944 &       0.230 &  0.092 &          0.062 &     1.057 &    -0.857 &    0.160 \\
    2 &         Evolved HMM 5 &           2.944 &       0.205 &  0.110 &          0.062 &     0.796 &    -0.448 &    0.294 \\
    3 &     PSO Gambler 2\_2\_2 &           2.913 &       0.204 &  0.128 &          0.062 &     0.899 &    -0.508 &    0.255 \\
    4 &      PSO Gambler Mem1 &           2.908 &       0.211 &  0.128 &          0.062 &     0.705 &    -0.186 &    0.333 \\
    5 &     PSO Gambler 1\_1\_1 &           2.906 &       0.221 &  0.145 &          0.062 &     0.737 &    -0.209 &    0.296 \\
    7 &         Evolved ANN 5 &           2.893 &       0.225 &  0.185 &          0.062 &     0.804 &    -0.608 &    0.334 \\
   31 &             ZD-GTFT-2 &           2.721 &       0.000 &  0.081 &          0.062 &     0.786 &    -0.502 &    0.289 \\
   45 &              ZD-GEN-2 &           2.689 &       0.016 &  0.096 &          0.062 &     0.694 &    -0.227 &    0.358 \\
   69 &           Tit For Tat &           2.638 &       0.000 &  0.157 &          0.062 &     0.773 &    -0.507 &    0.301 \\
   75 &                Grumpy &           2.630 &       0.075 &  0.100 &          0.062 &     0.978 &    -1.438 &    0.245 \\
   88 &   Win-Stay Lose-Shift &           2.616 &       0.099 &  0.122 &          0.062 &     1.172 &    -4.501 &    0.027 \\
  103 & Eventual Cycle Hunter &           2.565 &       0.067 &  0.052 &          0.062 &     0.728 &    -0.338 &    0.357 \\
  127 &              Adaptive &           2.272 &       0.500 &  0.314 &         -1.000 &     0.084 &     2.171 &    0.010 \\
  168 &              ZD-SET-2 &           1.975 &       0.451 &  0.418 &          2.407 &     0.081 &     5.244 &    0.006 \\
  169 &                 Bully &           1.970 &       0.381 &  0.141 &         -1.000 &     1.373 &    -2.221 &    0.140 \\
  179 &            Alternator &           1.945 &       0.392 &  0.259 &          3.857 &     1.332 &    -1.021 &    0.120 \\
  181 &              Negation &           1.941 &       0.356 &  0.141 &         -1.000 &     1.470 &    -3.204 &    0.083 \\
  182 &    CollectiveStrategy &           1.931 &       0.915 &  0.762 &         -2.888 &     0.085 &     6.082 &    0.028 \\
  183 &             Cycler DC &           1.931 &       0.324 &  0.256 &          3.857 &     1.279 &    -0.900 &    0.140 \\
  188 &              Hopeless &           1.908 &       0.352 &  0.048 &          1.833 &     2.247 &    -1.694 &    0.139 \\
  194 &        Gradual Killer &           1.892 &       0.354 &  0.367 &          0.062 &     0.254 &     1.669 &    0.106 \\
  196 &            Aggravater &           1.879 &       0.930 &  0.739 &         -2.889 &     0.163 &     2.951 &    0.066 \\
  200 &           ZD-Extort-2 &           1.821 &       0.851 &  0.652 &          2.005 &     0.019 &     5.435 &    0.009 \\
  201 &           ZD-Extort-4 &           1.820 &       0.865 &  0.697 &          4.003 &     0.021 &     3.677 &    0.005 \\
  202 &            ZD-Extort3 &           1.810 &       0.862 &  0.687 &          3.028 &     0.015 &     5.066 &    0.005 \\
  203 &              Defector &           1.808 &       0.929 &  0.800 &         -2.889 &     0.059 &     0.000 &    0.000 \\
  204 &             Handshake &           1.806 &       0.870 &  0.737 &         -2.888 &     0.126 &     3.825 &    0.083 \\
\hline
\end{tabular}
    \end{adjustwidth}
\end{table}


\begin{figure}[!htbp]
    \centering
    \caption{{\bf Skew of \(\SSe\) for all strategies considered over all opponents.
        A similar conclusion to that of
        Fig~\ref{fig:sserror_in_stewart_plotkin} can be made: the strategies
        that score highly have a negatively skewed \(\SSe\) highlighting their
        ability to adapt to their opponent. The auxiliary materials include a
        version of this graphic with strategy names.}}
        \label{fig:sserror_in_std}
\end{figure}

\subsection*{Evolutionary dynamics}

In the original work introducing ZD strategies~\cite{Press2012}, effectiveness
in evolutionary settings was already considered. Since then, most work
surrounding these strategies considers their performance in evolutionary
settings. Examples include~\cite{adami2013evolutionary, hilbe2018partners,
Hilbe2013, hilbe2013adaptive, hilbe2015partners, ichinose2018zero, Moran1707}.
The main motivation for this consideration is to gain insights on to how
behaviours might arise but also whether or not they are stable in various
settings such as social and biological interactions. Most of such work
considers the space of memory-one strategies alone. In contrast, this paper considers a wider
strategy space and two models of evolution are investigated: the
continuous replicator dynamics and the discrete Moran process.

\subsubsection*{Replicator Dynamics}

From the large number of interactions a payoff matrix \(S\) can be measured
where \(S_{ij}\) denotes the score (using standard values of \((R, S, T, P) =
(3, 0, 5, 1)\)) of the \(i\)th strategy against the \(j\)th strategy. Given a
population of strategies represented by \(\boldsymbol{\gamma}\) where \(\gamma_i\)
denotes the proportion of the population occupied by the \(i\)th strategy, the
fitness landscape under evolution can be considered. This is traditionally done
using the replicator equation, describing the evolution of the population
under selection:

\begin{equation}\label{eqn:replicator_dynamics}
    \frac{d \gamma_i}{dt} = \gamma_i ((S\boldsymbol{\gamma})_i - x^T S \boldsymbol{\gamma})
\end{equation}

Eq (\ref{eqn:replicator_dynamics}) is solved numerically for an initial
population with a uniform distribution of the strategies. This is done using an
integration technique described in~\cite{Petzold1983} until a stationary vector
\(\gamma=s\) is found.
Fig~\ref{fig:replicator_dynamics} shows the stationary probabilities for each
strategy ranked by score.
It is clear to see that
only the high ranking strategies survive the evolutionary process (in fact,
only 39 have a stationary
probability value greater than \(10 ^ {-2}\)).

\begin{figure}[!htbp]
    \centering
    \caption{{\bf Stationary distribution of the replicator dynamics
    (\ref{eqn:replicator_dynamics}): strategies are ordered by score (as given
    in Table~\ref{tbl:overall_summary_results}). The 2
    strategies with the highest stationary probability are:
    EvolvedLookerUp2\_2\_2 and Evolved HMM 5.
    Note that
    strategies that make use of the knowledge of the length of the game are
    removed from this analysis as they have an evolutionary advantage.}}
    \label{fig:replicator_dynamics}
\end{figure}

Fig~\ref{fig:compare-evolutionary-dynamics-to-sserror} plots the mean and
skew (a standard statistical measure on a distribution) of \(\SSe\) against the
stationary probabilities \(\textbf{s}\) of (\ref{eqn:replicator_dynamics}). Strategies
that perform strongly according to Eq (\ref{eqn:replicator_dynamics}) seem
to be strategies that have a negative skew of \(\SSe\): indicating that they
often have a high value of \(\SSe\) (i.e. do not act extortionately) but have a
long left tail allowing them to adapt when necessary. A general linear model
obtained using recursive feature elimination is shown in
Table~\ref{tbl:compare-evolutionary-dynamics-to-sserror} with stronger
predictive power and confirming these conclusions.

\begin{figure}[!hbtp]
    \centering
    \caption{{\bf Mean, variance and skew of \(\SSe\) versus the stationary
    probabilities of (\ref{eqn:replicator_dynamics}). The plot of the skew
    clearly shows that all high probabilities have a negative skew.}}
    \label{fig:compare-evolutionary-dynamics-to-sserror}
\end{figure}

\begin{table}[!hbtp]
    %\begin{adjustwidth}{-2.25in}{0in} % Comment out/remove adjustwidth environment if table fits in text column.
    \centering
        \caption{{\bf General linear model predicting the stationary probability as a
    function of the mean, median and variance of the \(\SSe\). This shows that strategies with a low mean
    and high median are more likely to survive the evolutionary dynamics. This
    corresponds to negatively skewed distributions of \(\SSe\) which again
        highlights the importance of adaptability.}}
    \label{tbl:compare-evolutionary-dynamics-to-sserror}
    %\end{adjustwidth}
\begin{tabular}{lclc}
\thickhline
\textbf{Dep. Variable:}    &      $s_i$       & \textbf{  R-squared:         } &    0.648  \\
\textbf{Model:}            &       OLS        & \textbf{  Adj. R-squared:    } &    0.642  \\
\textbf{Method:}           &  Least Squares   & \textbf{  F-statistic:       } &    117.0  \\
                           &                  & \textbf{  Prob (F-statistic):} & 5.00e-43  \\
                           &                  & \textbf{  Log-Likelihood:    } &   851.41  \\
\textbf{No. Observations:} &         195      & \textbf{  AIC:               } &   -1695.  \\
\textbf{Df Residuals:}     &         191      & \textbf{  BIC:               } &   -1682.  \\
\textbf{Df Model:}         &           3      & \textbf{                     } &           \\
\textbf{Covariance Type:}  &    nonrobust     & \textbf{                     } &           \\
\hline
\end{tabular}
%\caption{OLS Regression Results}
\begin{tabular}{lcccccc}
\hline
                                & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$>$$|$t$|$} & \textbf{[0.025} & \textbf{0.975]}  \\
\thickhline
\textbf{const}                  &       0.0007  &        0.001     &     1.137  &         0.257        &       -0.000    &        0.002     \\
\textbf{('SSE', 'mean')}   &      -0.0134  &        0.002     &    -8.369  &         0.000        &       -0.017    &       -0.010     \\
\textbf{('SSE', 'median')} &       0.0139  &        0.001     &    10.433  &         0.000        &        0.011    &        0.017     \\
\textbf{('SSE', 'var')}    &       0.0069  &        0.003     &     2.402  &         0.017        &        0.001    &        0.013     \\
\hline
\end{tabular}
\begin{tabular}{lclc}
\thickhline
\textbf{Omnibus:}       & 17.190 & \textbf{  Durbin-Watson:     } &    1.664  \\
\textbf{Prob(Omnibus):} &  0.000 & \textbf{  Jarque-Bera (JB):  } &   25.453  \\
\textbf{Skew:}          &  0.530 & \textbf{  Prob(JB):          } & 2.97e-06  \\
\textbf{Kurtosis:}      &  4.418 & \textbf{  Cond. No.          } &     23.7  \\
\hline
\end{tabular}
\end{table}


Fig~\ref{fig:sserror_distribution_for_selection_of_strategies} shows the
distribution of the \(\SSe\) for three selected strategies. It is evident that
Extort-2 almost always has the same low value of \(\SSe\) against all opponents
(which gives a positively skewed distribution), whereas EvolvedLookerUp2\_2\_2
and Tit For Tat have a wider distribution of values depending on the opponent
(which gives a negatively skewed distribution).

\begin{figure}[!hbtp]
    \centering
    \caption{{\bf Distribution of \(\SSe\) values for 3 selected strategies. The
    first two distributions are negatively skewed and the third has a positive
    skew.}}
    \label{fig:sserror_distribution_for_selection_of_strategies}
\end{figure}

\subsubsection*{Finite Population Dynamics: Moran Process}

The Moran Process is an evolutionary model of evolutionary in a finite
population. Of specific interest is the probability of a single 
individual entrant to a population taking over the population.
This is referred to as the fixation probability denoted by \(\kappa_1\).
In~\cite{Moran1707} a large data set of pairwise fixation probabilities in the
Moran process is made available at~\cite{vincent_knight_2017_1040129}.
Fig~\ref{fig:compare-fixation-to-sserror} shows linear models fitted to three
summary measures of \(\SSe\) and the mean (over population size \(N\) and
opponents) value of \(\kappa_1\cdot N\). This
specific measure of fixation is chosen as \(\kappa_1\) is usually compared to
the neutral fixation probability of \(1 / N\).  As was noted
in~\cite{Moran1707}, the specific case of \(N=2\) differs from all other
population sizes which is why it is presented in isolation.  We note that there
is a significant relationship between the skew of \(\SSe\) and the ability for a
strategy to become fixed.  A general linear model obtained through recursive
feature elimination is shown in Table~\ref{tbl:compare-fixation-to-sserror}
which confirms the conclusions.

\begin{figure}[!hbtp]
    \centering
    \caption{{\bf The mean, variance and skew of
    \(\SSe\) against the normalised pairwise fixation probabilities
    from~\cite{Moran1707} (for a given strategy averaged over all opponents and
    population sizes). The clustering either side of a value of skew equal to
    0 show that strategies with above neutral
    fixation (\(N\cdot x_1>1\)) negative skew.}}
    \label{fig:compare-fixation-to-sserror}
\end{figure}

\begin{table}[!hbtp]
    %\begin{adjustwidth}{-2.25in}{0in} % Comment out/remove adjustwidth environment if table fits in text column.
    \centering
        \caption{{\bf General linear model predicting the mean fixation probability as a
    function of the mean, median and variance of the \(\SSe\). This shows that strategies with a high mean
        and low median are likely to be evolutionarily stable. This corresponds
        to negatively skewed distributions of \(\SSe\) which again highlights
        the importance of adaptability.}}
    \label{tbl:compare-fixation-to-sserror}
\begin{tabular}{lclc}
\hline
\textbf{Dep. Variable:}    &       mean       & \textbf{  R-squared:         } &    0.319  \\
\textbf{Model:}            &       OLS        & \textbf{  Adj. R-squared:    } &    0.310  \\
\textbf{Method:}           &  Least Squares   & \textbf{  F-statistic:       } &    36.53  \\
                           &                  & \textbf{  Prob (F-statistic):} & 9.74e-14  \\
                           &                  & \textbf{  Log-Likelihood:    } &  -42.272  \\
\textbf{No. Observations:} &         159      & \textbf{  AIC:               } &    90.54  \\
\textbf{Df Residuals:}     &         156      & \textbf{  BIC:               } &    99.75  \\
\textbf{Df Model:}         &           2      & \textbf{                     } &           \\
\textbf{Covariance Type:}  &    nonrobust     & \textbf{                     } &           \\
\hline
\end{tabular}
\begin{tabular}{lcccccc}
\hline
                                & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$>$$|$t$|$} & \textbf{[0.025} & \textbf{0.975]}  \\
\thickhline
\textbf{const}                  &       1.2815  &        0.056     &    22.993  &         0.000        &        1.171    &        1.392     \\
\textbf{('SSE', 'mean')}   &      -1.0620  &        0.145     &    -7.323  &         0.000        &       -1.348    &       -0.776     \\
\textbf{('SSE', 'median')} &       0.9037  &        0.106     &     8.535  &         0.000        &        0.695    &        1.113     \\
\hline
\end{tabular}
\begin{tabular}{lclc}
\hline
\textbf{Omnibus:}       &  2.302 & \textbf{  Durbin-Watson:     } &    1.716  \\
\textbf{Prob(Omnibus):} &  0.316 & \textbf{  Jarque-Bera (JB):  } &    1.850  \\
\textbf{Skew:}          & -0.199 & \textbf{  Prob(JB):          } &    0.397  \\
\textbf{Kurtosis:}      &  3.348 & \textbf{  Cond. No.          } &     11.2  \\
\hline
\end{tabular}
    %\end{adjustwidth}
\end{table}


These findings confirm the work of~\cite{Moran1707} in which sophisticated
strategies resist evolutionary invasion of shorter memory strategies. This also
confirms the work of~\cite{adami2013evolutionary, hilbe2015partners} which
proved that ZD strategies where not evolutionarily stable due to the fact that
they score poorly against themselves.

The work also provides strong evidence to the importance of adaptability:
strategies that offer a variety of behaviours corresponding to a higher standard
deviation of \(\SSe\) are significantly more likely to survive the
evolutionary process. This corresponds to the following quote
of~\cite{darwin1869origin}:

\begin{quote}

    \textit{``It is not the most intellectual of the species that survives; it is not the
strongest that survives; but the species that survives is the one that is able
to adapt to and to adjust best to the changing environment in which it finds
itself.''
}

\end{quote}

\section*{Discussion}

This work defines an approach to measure whether or not a player is using an
extortionate strategy as defined in~\cite{Press2012}, or a strategy that behaves
similarly, broadening the definition of extortionate behavior. All extortionate
strategies have been classified as lying on a triangular plane. This rigorous
classification fails to be robust to small measurement error, thus a statistical
approach is proposed approximating the solution of a linear system.
This method
was applied to a large number of pairwise interactions.

The work of~\cite{Press2012}, while showing that a clever approach to taking
advantage of another memory-one strategy exists, is not the full story.
Though the elegance of this result is very attractive, just as the simplicity of
the victory of Tit For Tat in Axelrod's original tournaments was, it is
incomplete and in the author's opinions, has been oversimplified and
overgeneralized in subsequent work. Extortionate strategies achieve a high
number of wins but they do generally not achieve a high score and fail to be
evolutionarily stable.

Rather, more sophisticated strategies are able to adapt to a variety of opponents
and act extortionately only against weaker strategies while cooperating with
like-minded strategies that are not susceptible to extortion. This adaptability
may be key to maintaining sustained cooperation, as some of these strategies
emerged naturally from evolutionary processes trained to maximize payoff in
IPD tournaments and fixation in population dynamics.

Following Axelrod's seminal work~\cite{Axelrod1980, Axelrod1980a}, it was
commonly thought that evolutionary cooperation required strategies that followed
a simple set of rules. The discovery/definition of extortionate
strategies~\cite{Press2012} seemingly showed that complex strategies could be
taken advantage of. In this manuscript it has been shown that not only is it
possible to detect and prevent extortionate behaviour but that more complex
strategies can be evolutionary stable. The complex strategies in question were
obtained through reinforcement learning approaches~\cite{Harper2017, Moran1707}.
Thus, this demonstrates that it is possible to recognise extortion, both
theoretically using \(\SSe\) but also that this ability can develop through
reinforcement learning. It seems human difficulty in directly developing
effective complex strategies has been incorrectly generalized to a weakness
in complex strategies themselves, which is demonstrable not the case. In fact,
complex strategies can be the most effective against a diverse set of opponents.


A possible future research direction would be applying and or extending the
methodology proposed here to consider other theoretic models of control of
opponent utility such as~\cite{Akin2015, hao2018, chen2022}. There are however,
various potential immediate applications for \(\SSe\), one of which could be to
devise an agent that learns during the interactions with another agent.
Fig~\ref{fig:learning_sse} shows the average \(\SSe\) value over a number of
iterations over a number of repetitions. More investigation would be required
but in some cases it seems that a large number of interactions would be required
to gain certainly about the play of an agent. This approach seems to be in
opposition of some of the trained strategies of~\cite{Harper2017} which are
known to learn from early interactions and adapt their play.

\begin{figure}[!htbp]
    \centering
    \caption{{\bf The average \(\SSe\) of a few strategies over a number of
    repetitions and a number of turns. The EvolvedLookerUp2\_2\_2 stragey is
    recognisably not a ZD strategy after 10 turns against both opponents.  When
    playing against the EvolvedLookerUp2\_2\_2 the generous ZD strategy ZDSet2
    is also quickly recognisable as a ZD strategy after approximately 10 turns.
    Interestingly, while the other ZD strategy, ZD-Extort-2 v2, is clearly a
    acting as a ZD strategy early on against both opponents it would take longer
    to confidently recognise that that ZDSet2 is a ZD strategy.
        }}
    \label{fig:learning_sse}
\end{figure}

In closing, the authors wish to emphasize the role of comprehensive simulations to temper
theoretical results from overgeneralization, and perhaps more importantly, the
ability of simulations to provide insights that are difficult to obtain from theory.


\section*{Supporting information}

% Include only the SI item label in the paragraph heading. Use the \nameref{label} command to cite SI items in the text.
\paragraph*{S1 Appendix.}
\label{proof_of_algebraic_condition_for_extortionate_strategies}
{\bf Proof of algebraic condition for extortionate strategies.} 

\paragraph*{S2 Appendix.}
\label{sse_error_in_std_for_auxiliary}
{\bf Skew of SSe for all strategies.} The skew for all the strategies in the larger tournament.

\paragraph*{S3 Appendix.}
\label{list_of_strategies}
{\bf List of all strategies}

\section*{Acknowledgements}

The following open source software libraries were used in this research:

\begin{itemize}
    \item The Axelrod ~\cite{Knight2016, Knight2018} library (IPD strategies and
        tournaments).
    \item The sympy library~\cite{Meurer2017} (verification of all symbolic
        calculations).
    \item The matplotlib~\cite{Droettboom2018} library (visualisation).
    \item The pandas~\cite{Structures2010}, dask~\cite{Dask2016} and
        NumPy~\cite{Oliphant2015} libraries (data manipulation).
    \item The SciPy~\cite{Jones2001} library (numerical integration of the
        replicator equation).
\end{itemize}

This work was performed using the computational facilities of the Advanced
Research Computing @ Cardiff (ARCCA) Division, Cardiff University.

\nolinenumbers

\begin{thebibliography}{10}

\bibitem{Axelrod1980}
Axelrod R.
\newblock Effective Choice in the Prisoner{\textquotesingle}s Dilemma.
\newblock Journal of Conflict Resolution. 1980;24(1):3--25.
\newblock doi:{10.1177/002200278002400101}.

\bibitem{Axelrod1980a}
Axelrod R.
\newblock More Effective Choice in the Prisoner{\textquotesingle}s Dilemma.
\newblock Journal of Conflict Resolution. 1980;24(3):379--403.
\newblock doi:{10.1177/002200278002400301}.

\bibitem{glynatsi2021bibliometric}
Glynatsi NE, Knight VA.
\newblock A bibliometric study of research topics, collaboration, and
  centrality in the iterated prisoner’s dilemma.
\newblock Humanities and Social Sciences Communications. 2021;8(1):1--12.

\bibitem{Press2012}
Press WH, Dyson FJ.
\newblock Iterated Prisoner{\textquotesingle}s Dilemma contains strategies that
  dominate any evolutionary opponent.
\newblock Proceedings of the National Academy of Sciences.
  2012;109(26):10409--10413.
\newblock doi:{10.1073/pnas.1206569109}.

\bibitem{hilbe2015partners}
Hilbe C, Traulsen A, Sigmund K.
\newblock Partners or rivals? Strategies for the iterated prisoner's dilemma.
\newblock Games and economic behavior. 2015;92:41--52.

\bibitem{adami2013evolutionary}
Adami C, Hintze A.
\newblock Evolutionary instability of zero-determinant strategies demonstrates
  that winning is not everything.
\newblock Nature communications. 2013;4:2193.

\bibitem{hilbe2018partners}
Hilbe C, Chatterjee K, Nowak MA.
\newblock Partners and rivals in direct reciprocity.
\newblock Nature human behaviour. 2018;2(7):469--477.

\bibitem{Hilbe2013}
Hilbe C, Nowak MA, Sigmund K.
\newblock Evolution of extortion in Iterated Prisoner{\textquotesingle}s
  Dilemma games.
\newblock Proceedings of the National Academy of Sciences.
  2013;110(17):6913--6918.
\newblock doi:{10.1073/pnas.1214834110}.

\bibitem{hilbe2013adaptive}
Hilbe C, Nowak MA, Traulsen A.
\newblock Adaptive dynamics of extortion and compliance.
\newblock PlOS ONE. 2013;8(11):e77886.

\bibitem{ichinose2018zero}
Ichinose G, Masuda N.
\newblock Zero-determinant strategies in finitely repeated games.
\newblock Journal of theoretical biology. 2018;438:61--77.

\bibitem{Moran1707}
Knight V, Harper M, Glynatsi NE, Campbell O.
\newblock Evolution Reinforces Cooperation with the Emergence of
  Self-Recognition Mechanisms: an empirical study of the Moran process for the
  iterated Prisoner's dilemma; 2017.

\bibitem{lee2015art}
Lee C, Harper M, Fryer D.
\newblock The art of war: Beyond memory-one strategies in population games.
\newblock PloS one. 2015;10(3):e0120625.

\bibitem{becks2019extortion}
Becks L, Milinski M.
\newblock Extortion strategies resist disciplining when higher competitiveness
  is rewarded with extra gain.
\newblock Nature communications. 2019;10(1):783.

\bibitem{Knight2016}
Knight V, Campbell O, Harper M, Langner KM, Campbell J, Campbell T, et~al.
\newblock An Open Framework for the Reproducible Study of the Iterated
  Prisoner's Dilemma.
\newblock Journal of Open Research Software. 2016;4.
\newblock doi:{10.5334/jors.125}.

\bibitem{Knight2018}
Knight V, Campbell O, Marc, {Eric-S-S}, {VSN Reddy Janga}, Campbell J, et~al..
  Axelrod-Python/Axelrod: V4.2.0; 2018.

\bibitem{Wilson2014}
Wilson G, Aruliah DA, Brown CT, Hong NPC, Davis M, Guy RT, et~al.
\newblock Best Practices for Scientific Computing.
\newblock {PLOS} Biology. 2014;12(1):e1001745.
\newblock doi:{10.1371/journal.pbio.1001745}.

\bibitem{vincent_knight_2018_1297075}
Knight V. {Raw data for: "Suspicion: Recognising and evaluating the
  effectiveness of extortion in the Iterated Prisoner's Dilemma"}; 2018.
\newblock Available from: \url{https://doi.org/10.5281/zenodo.1297075}.

\bibitem{vincent_knight_2019_2598534}
Knight V. {Source code for paper on recognising zero determinant strategies};
  2019.
\newblock Available from: \url{https://doi.org/10.5281/zenodo.2598534}.

\bibitem{Stewart2012}
Stewart AJ, Plotkin JB.
\newblock Extortion and cooperation in the Prisoner{\textquotesingle}s Dilemma.
\newblock Proceedings of the National Academy of Sciences.
  2012;109(26):10134--10135.
\newblock doi:{10.1073/pnas.1208087109}.

\bibitem{Rand2013}
Rand DG, Nowak MA.
\newblock Human cooperation.
\newblock Trends in Cognitive Sciences. 2013;17(8):413--425.
\newblock doi:{10.1016/j.tics.2013.06.003}.

\bibitem{kutner2004applied}
Kutner MH, Nachtsheim C, Neter J.
\newblock Applied linear regression models.
\newblock McGraw-Hill/Irwin; 2004.

\bibitem{rao1973linear}
Rao CR.
\newblock Linear statistical inference and its applications. vol.~2.
\newblock Wiley New York; 1973.

\bibitem{wakefield2013bayesian}
Wakefield J.
\newblock Bayesian and frequentist regression methods.
\newblock Springer Science \& Business Media; 2013.

\bibitem{sep-prisoner-dilemma}
Kuhn S.
\newblock {Prisoner’s Dilemma}.
\newblock In: Zalta EN, editor. The {Stanford} Encyclopedia of Philosophy.
  {W}inter 2019 ed. Metaphysics Research Lab, Stanford University; 2019.

\bibitem{Harper2017}
Harper M, Knight V, Jones M, Koutsovoulos G, Glynatsi NE, Campbell O.
\newblock Reinforcement learning produces dominant strategies for the Iterated
  Prisoner's Dilemma.
\newblock {PLOS} {ONE}. 2017;12(12):e0188046.
\newblock doi:{10.1371/journal.pone.0188046}.

\bibitem{Petzold1983}
Petzold L.
\newblock Automatic Selection of Methods for Solving Stiff and Nonstiff Systems
  of Ordinary Differential Equations.
\newblock {SIAM} Journal on Scientific and Statistical Computing.
  1983;4(1):136--148.
\newblock doi:{10.1137/0904010}.

\bibitem{vincent_knight_2017_1040129}
Knight V, Harper M, Glynatsi NE. {Data for: Evolution Reinforces Cooperation
  with the Emergence of Self-Recognition Mechanisms: an empirical study of the
  Moran process for the iterated Prisoner's dilemma using reinforcement
  learning}; 2017.
\newblock Available from: \url{https://doi.org/10.5281/zenodo.1040129}.

\bibitem{darwin1869origin}
Darwin C.
\newblock ORIGIN OF SPECIES.
\newblock The Athenaeum. 1869;(2174):861--861.

\bibitem{Akin2015}
Akin E.
\newblock What you gotta know to play good in the iterated prisoner’s
  dilemma.
\newblock Games. 2015;6(3):175--190.

\bibitem{hao2018}
Hao D, Li K, Zhou T.
\newblock Payoff Control in the Iterated Prisoner's Dilemma.
  2018;doi:{10.48550/ARXIV.1807.06666}.

\bibitem{chen2022}
Chen X, Fu F. Outlearning Extortioners by Fair-minded Unbending Strategies;
  2022.
\newblock Available from: \url{https://arxiv.org/abs/2201.04198}.

\bibitem{Meurer2017}
Meurer A, Smith CP, Paprocki M, {\v{C}}ert{\'{\i}}k O, Kirpichev SB, Rocklin M,
  et~al.
\newblock {SymPy}: symbolic computing in Python.
\newblock {PeerJ} Computer Science. 2017;3:e103.
\newblock doi:{10.7717/peerj-cs.103}.

\bibitem{Droettboom2018}
Droettboom M, Caswell TA, Hunter J, Firing E, Nielsen JH, Lee A, et~al..
  Matplotlib/Matplotlib V2.2.2; 2018.

\bibitem{Structures2010}
Structures D, for Statistical, in~Python C, McKinney W.
\newblock PROC. OF THE 9th PYTHON IN SCIENCE CONF. (SCIPY 2010); 2010.

\bibitem{Dask2016}
{Dask Development Team}. Dask: Library for dynamic task scheduling; 2016.
\newblock Available from: \url{http://dask.pydata.org}.

\bibitem{Oliphant2015}
Oliphant TE.
\newblock Guide to NumPy: 2nd Edition.
\newblock CreateSpace Independent Publishing Platform; 2015.
\newblock Available from:
  \url{https://www.amazon.com/Guide-NumPy-Travis-Oliphant-PhD/dp/151730007X?SubscriptionId=0JYN1NVW651KCA56C102&tag=techkie-20&linkCode=xm2&camp=2025&creative=165953&creativeASIN=151730007X}.

\bibitem{Jones2001}
Jones E, Oliphant T, Peterson P, et~al.. {SciPy}: Open source scientific tools
  for {Python}; 2001--.
\newblock Available from: \url{http://www.scipy.org/}.

\end{thebibliography}

\end{document}

